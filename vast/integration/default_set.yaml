config-file: vast.yaml

fixtures:
  BiTopologyTester:
    enter: | # python
      node0 = Server(self.cmd,
                     ['-e', f':{VAST_PORT}', '-i', 'node0', 'start'],
                     work_dir, name='node0', port=VAST_PORT,
                     config_file=self.config_file')
      node1 = Server(self.cmd,
                     ['-e', ':42124',
                      '-i', 'node1', 'start'],
                     work_dir, name='node1', port=42124,
                     config_file=self.config_file)
      cmd += ['-e', f':{VAST_PORT}']

    exit: | # python
      node0.stop()
      node1.stop()

  ExampleConfigFileTester:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f':{VAST_PORT}', '-i', 'node', 'start'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file='vast.yaml.example')
      cmd += ['-e', f':{VAST_PORT}']

    exit: | # python
      node.stop()

  ServerTester:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f':{VAST_PORT}', '-i', 'node', 'start'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file=self.config_file)
      cmd += ['-e', f':{VAST_PORT}']

    exit: | # python
      node.stop()

  AgingTester:
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f':{VAST_PORT}',
                     '-i', 'node',
                     '--aging-frequency=2000min',
                     '--aging-query=:addr == 192.168.1.104',
                     'start'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file=self.config_file)
      cmd += ['-e', f':{VAST_PORT}']

    exit: | # python
      node.stop()

  DiskMonitorTester:
    # Values are chosen such that one import of `conn.log.gz`
    # results in one partition ~600KiB and an additional ~700KiB
    # of segment data.
    enter: | # python
      node = Server(self.cmd,
                    ['-e', f':{VAST_PORT}',
                     '-i', 'node',
                     '--max-partition-size=8000',
                     '--max-segment-size=1',
                     'start',
                     '--disk-budget-check-interval=1',
                     '--disk-budget-high=5MiB',
                     '--disk-budget-low=2MiB'],
                    work_dir, name='node', port=VAST_PORT,
                    config_file=self.config_file)
      cmd += ['-e', f':{VAST_PORT}']

    exit: | # python
      node.stop()

tests:
  JSON schema inference:
    tags: [schema, infer, json]
    steps:
      - command: infer
        input: data/json/empty-object.json
      - command: infer
        input: data/json/basic-types.json
      - command: infer
        input: data/json/string-types.json
      - command: infer
        input: data/json/array-types.json
      - command: infer
        input: data/json/nested-object.json
  Conn log counting:
    tags: [node, counting, zeek]
    steps:
      - command: -N --max-partition-size=64 import -b zeek
        input: data/zeek/conn.log.gz
      - command: -N count ":addr == 192.168.1.104"
      - command: -N count -e ":addr == 192.168.1.104"
      - command: -N count "resp_p == 80"
      - command: -N count "resp_p != 80"
      - command: -N count 861237
  Node Zeek conn log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N export ascii 'resp_h == 192.168.1.104'
      - command: -N export ascii 'orig_bytes > 1k && orig_bytes < 1Ki'
      - command: -N export ascii ':string == "OrfTtuI5G4e" || :count == 67' # TODO: make this :port == 67 once we support built-in type aliases
      - command: -N export ascii '#type == "zeek.conn" && resp_h == 192.168.1.104'
      - command: '-N export ascii "#type != \"zeek.conn\" && #type != \"vast.metrics\""'
      - command: -N export ascii '#type != "foobar" && resp_h == 192.168.1.104'
      - command: -N get --format=json 1 42 7777 9999999999 122
      - command: -N get --format=csv 99 99 99
  Node Zeek multiple imports:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N import zeek
        input: data/zeek/dns.log.gz
      - command: -N export ascii 'resp_h == 192.168.1.104'
      - command: -N export ascii 'zeek.conn.id.resp_h == 192.168.1.104'
      - command: '-N count "#timestamp >= 1970-01-01 && #type != \"vast.metrics\""'
      - command: -N count '#type == "zeek.conn"'
  Node Zeek dns log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/dns.log.gz
      - command: -N export ascii 'resp_h == 192.168.1.104'
      - command: -N export zeek 'resp_h == 192.168.1.104'
        transformation: awk '!/^#(open|close)/'
  Node Zeek http log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/http.log.gz
      - command: -N export ascii 'resp_h == 216.240.189.196'
  Node Zeek snmp log:
    tags: [node, import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/snmp.log.gz
      - command: -N export ascii 'duration >= 3s'
  Node Zeek JSON:
    tags: [node, import-export, zeek, json]
    steps:
      - command: -N import -b zeek-json
        input: data/zeek/zeek.json
      - command: '-N export json "\"zeek\" in #type"'
  Export json:
    tags: [json, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N export json 'resp_h == 192.168.1.104'
  Malformed Query:
    tags: [export]
    fixture: ServerTester
    steps:
      - command: export json 'yo that is not a query'
        expected_result: error
      - command: and that is not a command
        expected_result: error
  Server Zeek conn log:
    tags: [server, import-export, zeek]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'resp_h == 192.168.1.104'
      # import some more to make sure accounting data is in the system.
      - command: import -b --batch-size=10 zeek
        input: data/zeek/conn.log.gz
      - command: import -b --batch-size=1000 zeek
        input: data/zeek/conn.log.gz
      - command: import -b --batch-size=100000 zeek
        input: data/zeek/conn.log.gz
      - command: import -b --batch-size=1 -n 242 zeek
        input: data/zeek/conn.log.gz
      - command: status --detailed
        transformation: jq '.index.statistics.layouts | del(."vast.metrics")'
  Server Zeek multiple imports:
    tags: [server, import-export, zeek]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: import -b zeek
        input: data/zeek/dns.log.gz
      - command: export ascii 'resp_h == 192.168.1.104'
      - command: export ascii 'zeek.conn.id.resp_h == 192.168.1.104'
      - command: 'count "#timestamp >= 1970-01-01 && #type != \"vast.metrics\""'
      - command: count '#type == "zeek.conn"'
  Table Slice Types:
    tags: [server, import-export, batch-encoding, zeek]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: import -b --batch-encoding=msgpack zeek
        input: data/zeek/conn.log.gz
      - command: count '#type == "zeek.conn"'
      - guard: version | jq -e '."Apache Arrow"'
      - command: import -b --batch-encoding=arrow zeek
        input: data/zeek/conn.log.gz
      - command: count '#type == "zeek.conn"'
  Query Operators:
    tags: [server, operator]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'conn.duration <= 1.0s'
      - command: export ascii 'duration >= 10.0s'
      - command: export ascii 'duration < 2.0s'
      - command: export ascii 'service  == "smtp"'
      - command: export ascii 'missed_bytes  != 0'
      - command: export ascii 'id.orig_h !in 192.168.1.0/24'
      - command: export ascii 'id.orig_h in fe80:5074:1b53:7e7::/64'
      - command: export ascii 'id.orig_h ni fe80:5074:1b53:7e7::/64'
  Expressions:
    tags: [server, expression]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'fe80::5074:1b53:7e7:ad4d || 169.254.225.22'
      - command: export ascii '"OrfTtuI5G4e" || fe80::5074:1b53:7e7:ad4d'
  Node Type Query:
    tags: [node, type, ch6104]
    steps:
      - command: -N import -n 20 zeek
        input: data/zeek/conn.log.gz
      - command: -N export ascii '#type == "zeek.conn"'
  Type Query:
    tags: [server, type, ch5404]
    fixture: ServerTester
    steps:
      - command: import -b -n 20 zeek
        input: data/zeek/conn.log.gz
      - command: export ascii '#type == "zeek.conn"'
  Node json zeek conn:
    tags: [node, import-export, zeek, json]
    steps:
      - command: -N import json -s @./misc/schema/zeek-conn.schema -t zeek.conn.custom
        input: data/json/conn.log.json.gz
      - command: -N export ascii 'duration > 6s'
      - command: -N export ascii '#timestamp >= 2011-08-15T03:48'
  Node suricata alert:
    tags: [node, import-export, suricata, eve]
    steps:
      - command: -N import suricata -r @./data/suricata/eve.json
      - command: -N export ascii 'src_ip == 147.32.84.165'
      - command: -N export csv '#type ~ /suricata.*/'
      - command: -N export zeek '#type ~ /suricata.alert/'

  Node argus csv:
    tags: [node, import-export, argus, csv]
    steps:
      - command: -N import csv -t argus.record
        input: data/csv/argus-M57-10k-pkts.csv.gz
      - command: -N export ascii 'State != "CON" && Dur > 4900ms'
      - command: -N export ascii 'Cause == "Status" && Dur > 1s'

  Manual 1:
    condition: version | jq -e '."PCAP"'
    tags: [examples]
    fixture: ServerTester
    steps:
      - command: import -b pcap
        input: data/pcap/example.pcap.gz
      - command: export ascii ':addr in 192.168.168.0/24'
      - command: export pcap 'sport > 60000 && src !in 192.168.168.0/8'

  Manual 1 node:
    condition: version | jq -e '."PCAP"'
    tags: [examples]
    steps:
      - command: -N import pcap
        input: data/pcap/example.pcap.gz
      - command: -N export ascii ':addr in 192.168.168.0/24'
      - command: -N export pcap 'sport > 60000 && src !in 192.168.168.0/8'
      - command: -N status --detailed
        transformation: jq '.index.statistics | del(."meta-index-bytes")'

  Manual 2:
    tags: [examples, disabled]
    fixture: BiTopologyTester
    steps:
      - command: peer 'localhost:42124'
      - command: status

  Importing VLAN traffic:
    condition: version | jq -e '."PCAP"'
    tags: [import, pcap]
    steps:
      - command: -N import pcap
        input: data/pcap/vlan.pcap.gz
      - command: -N status --detailed
        transformation: jq '.index.statistics | del(."meta-index-bytes")'

  Multi addr query:
    tags: [import-export, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N export ascii
        input: queries/multi_addr.txt

  Taxonomy queries:
    tags: [concepts, models]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/pcap/zeek/conn.log.gz
      - command: import -b suricata
        input: data/pcap/suricata/eve.json.gz
      - command: count "net.src.ip == 192.168.168.100"
      - command: count "net.connection == <192.168.168.100, _, 72.247.178.18, _, _>"
      # We omit the whitespace after the colon on purpose, otherwise pyyaml
      # thinks this is a key-value pair.
      - command: count 'net.connection == <net.src.ip:192.168.168.100, net.dst.port:80>'
      - command: count 'net.connection != <net.src.ip:192.168.168.100, net.dst.port:80>'
      - command: count "net.connection == <_, _, _, _, _>"
      - command: count "net.connection == <_, _, _, 80, _>"
      - command: count "net.connection != <_, _, _, 80, _>"

  Taxonomy dump:
    tags: [concepts, models]
    fixture: ServerTester
    steps:
      - command: dump concepts
      - command: dump --yaml models

  Pivot:
    tags: [pivot]
    steps:
      - command: -N import suricata
        input: data/pcap/suricata/eve.json.gz
      - command: -N import zeek
        input: data/pcap/zeek/conn.log.gz
      - command: -N import zeek
        input: data/pcap/zeek/files.log.gz
      - command: -N import zeek
        input: data/pcap/zeek/http.log.gz
      - command: -N import zeek
        input: data/pcap/zeek/packet_filter.log.gz
      - command: -N import zeek
        input: data/pcap/zeek/ssh.log.gz
      - command: -N pivot --format=zeek zeek.conn '#type ~ /suricata.*/ && dest_ip == 72.247.178.18'
        transformation: awk '!/^#(open|close)/'
      - command: -N pivot zeek.conn '#type ~ /zeek.http/ && method == "GET"'
        transformation: awk '!/^#(open|close)/'
      - guard: version | jq -e '."PCAP"'
      - command: -N import pcap
        input: data/pcap/example.pcap.gz
      - command: -N pivot --format=pcap pcap.packet 'dest_ip == 72.247.178.18'
        transformation: tcpdump -n -tt -r -
  Arrow:
    condition: version | jq -e '."Apache Arrow"'
    tags: [export, arrow]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N export -n 10 arrow '#type == "zeek.conn"'
        transformation: python @./misc/scripts/print-arrow.py
      - command: -N import suricata
        input: data/suricata/eve.json
      - command: -N export arrow '#type == "suricata.http"'
        transformation: python @./misc/scripts/print-arrow.py
  Import syslog:
    tags: [syslog, import]
    steps:
      - command: -N import syslog
        input: data/syslog/syslog.log
      - command: -N export ascii '#type == /syslog.*/'
  Explore Zeek:
    tags: [explore, zeek]
    fixture: ServerTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: explore --after=3min 'uid == "Yaix3QBP3Xg"'
      - command: explore --format=ascii --after=3min 'uid == "Yaix3QBP3Xg"'
  # The first queries are stress tests for the deduplication: The query
  # selects every event, and the huge timebox ensures that every event
  # is included in the context of every other event.
  Explore Suricata With Overlap:
    tags: [explore, suricata]
    steps:
      - command: -N import suricata
        input: data/suricata/eve.json
      - command: -N explore --before=10years --after=10years '#type == /suricata.*/'
      - command: -N explore --max-events=3 --before=10years --after=10years '#type == /suricata.*/'
        transformation: wc -l | tr -d ' '
      - command: -N explore --max-events-query=1 --before=10years --after=10years '#type == /suricata.*/'
        transformation: wc -l | tr -d ' '
      - command: -N explore --max-events-query=3 --before=0s --after=1ns '#type == /suricata.*/'
        transformation: wc -l | tr -d ' '
      - command: -N explore --max-events-query=1 --max-events-context=2 --before=10years --after=10years '#type == /suricata.*/'
        transformation: wc -l | tr -d ' '
  # TODO(ch15579): Re-enable this test once the slow zeek export is fixed.
  # Explore Zeek With Overlap:
  #   tags: [explore, zeek]
  #   steps:
  #     - command: -N import zeek
  #       input: data/zeek/conn.log.gz
  #     - command: -N explore --before=10000h --after=10000h '#type == /zeek.*/'
  Explore Zeek By Field:
    tags: [explore, zeek]
    steps:
      - command: -N import zeek
        input: data/zeek/conn.log.gz
      - command: -N import zeek
        input: data/zeek/dns.log.gz
      - command: -N explore --by='id.orig_h' --after=1s --before=0s 'zeek.dns.query == "survey.112.2o7.net"'
  Heterogenous JSONL import:
    tags: [import, json, sysmon]
    steps:
      - command: -N import json
        input: data/json/sysmon.json
      - command: -N status --detailed
        transformation: jq '.index.statistics.layouts | del(."vast.metrics")'
      - command: -N import suricata
        input: data/suricata/eve.json
      - command: -N export json '"®" in :string'
      - command: -N export json '#type ni "suricata"'

  Zeek conn log aging:
    tags: [import-export, aging, zeek]
    fixture: AgingTester
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'resp_h == 192.168.1.104'
      - command: send eraser run
      - command: export ascii 'resp_h == 192.168.1.104'
  Spawn source:
    tags: [import, spawn-source, zeek]
    fixture: ServerTester
    steps:
      - command: spawn source suricata -r @./data/suricata/eve.json
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: count '#type ~ /suricata.*/'
  Example config file:
    tags: [import-export, zeek]
    fixture: ExampleConfigFileTester
    config-file: vast.yaml.example
    steps:
      - command: import -b zeek
        input: data/zeek/conn.log.gz
      - command: export ascii 'net.app !in ["dns", "ftp", "http", "ssl"]'
  # The idea is import just enough to hit the high-water mark, wait, and then
  # verify that most of the partitions and segments were deleted by checking
  # the number of events left in the database. This test is a bit annoying as
  # the numbers of the fixture must be set up quite precisely to trigger the
  # disk monitor at the last import, but on the other hand it will also allow
  # us to catch changes that drastically change the space used by the on-disk
  # layout of the VAST data structures.
  # TODO: This is currently too unreliable, re-enable this once dcso/ch1927 is done.
  # Disk monitor:
  #   tags: [disk-quota]
  #   fixture: DiskMonitorTester
  #   steps:
  #     # We have to fix the batch-size to make sure the input arrives as a single
  #     # table slice in archive and index.
  #     - command: import --batch-size=8462 zeek
  #       input: data/zeek/conn.log.gz
  #     - command: import --batch-size=8462 zeek
  #       input: data/zeek/conn.log.gz
  #     - command: import --batch-size=8462 zeek
  #       input: data/zeek/conn.log.gz
  #     - command: import --batch-size=8462 zeek
  #       input: data/zeek/conn.log.gz
  #     # Table slices arent flushed until the next event arrives, so we top off
  #     # the import with a tiny final table slice and give the disk monitor time
  #     # to run.
  #     - command: import zeek
  #       input: data/zeek/snmp.log.gz
  #       transformation: sleep 4
  #     - command: count '#type == /zeek.*/'
